# Configuration for Pre-training with Time-Slice-as-Token

run_name: "pretrain_time_v1"
task_type: "pretrain"
base_results_dir: "./results"
device: "cuda"
seed: 123

data:
  input_seismogram_path: "./synthetic_data/resized/seismograms_whole_resized.npz"
  processed_data_dir: "processed_data"
  token_type: "time_slice" # Data will be permuted (N, Time, Receivers)
  train_split_ratio: 0.8

  # Slicing applied after permuting data to (N, Time, Receivers)
  # to achieve model input dimensions (167 time-slices x 118 receivers/time-slice).
  # Original script (pre_pretraining_time.py): seismogram_whole[:, 2:-3, :] then permute.
  # If input is (N, 43 rec, 255 time) -> slice to (N, 38 rec, 255 time) -> permute to (N, 255 time, 38 rec).
  # This means 255 time-slices (sequence_length), 38 receivers/slice (input_feature_dim).
  # This does NOT match model config below (167 sequence_length, 118 features).
  # The user MUST ensure resize.yaml and these slices create data of 167x118.
  time_sample_slice_start: null # Slice along time axis (becomes sequence_length). MUST BE 167.
  time_sample_slice_end: null
  receiver_slice_start: 2 # Slice along receiver axis (becomes input_feature_dim). MUST BE 118.
  receiver_slice_end: -3

  augmentation:
    multiply_factor: 15
    time_shift_ops: 2
    min_time_shift: -15
    max_time_shift: 15
  masking:
    mask_proportion: 0.15
    mask_token_prob: 0.8
    random_token_prob: 0.1

model: # BERT model configuration
  # Based on original pretraining_time.py for TIME-AS-TOKEN:
  input_feature_dim: 118 # Length of each time-slice token (num_receivers per slice)
  sequence_length: 167 # Number of time-slice tokens (num_time_slices)
  hidden_size: 256
  num_hidden_layers: 4
  num_attention_heads: 4
  intermediate_ffn_factor: 4
  position_embedding_type: "sincos"
  pre_ln: true
  attention_type: "default"
  add_alibi: false
  alibi_type: "nosym"
  fixed_slopes: false
  add_urpe: false
  embedding_type: "none"
  layer_norm_eps: 1.0e-12
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  hidden_act: "gelu"

training:
  output_dir: "training_output"
  batch_size: 256
  learning_rate: 5.0e-4
  epochs: 200
  patience_early_stopping: 200
  optimizer: "RAdam"
  loss_function: "MSELoss"
