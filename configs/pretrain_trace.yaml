# configs/pretrain_trace.yaml

run_name: "pretrain_trace_v1"
task_type: "pretrain"
base_results_dir: "./results"
device: "cuda"
seed: 123

# --- START: 이 부분의 구조를 수정합니다 ---
data:
  input_seismogram_path: "./synthetic_data/resized/seismograms_whole_resized.npz"
  processed_data_dir: "processed_data"
  token_type: "trace"
  train_split_ratio: 0.8
  receiver_slice_start: 2
  receiver_slice_end: -3
  time_sample_slice_start: null
  time_sample_slice_end: null

# 사전 학습에만 사용되는 파라미터를 위한 섹션 추가
pretrain_params:
  augmentation:
    multiply_factor: 15
    time_shift_ops: 2
    min_time_shift: -15
    max_time_shift: 15
  masking:
    mask_proportion: 0.15
    mask_token_prob: 0.8
    random_token_prob: 0.1
# --- END: 구조 수정 완료 ---

model:
  input_feature_dim: 167 # Length of each trace token (num_time_samples per trace)
  sequence_length: 118 # Number of trace tokens (num_traces)
  hidden_size: 256
  num_hidden_layers: 4
  num_attention_heads: 4
  intermediate_ffn_factor: 4 # original: num_hidden_ffn
  position_embedding_type: "sincos"
  pre_ln: true
  attention_type: "default" # As in pretraining.py's BertSelfAttention default
  add_alibi: false
  alibi_type: "nosym"
  fixed_slopes: false # For ALiBi
  add_urpe: false
  embedding_type: "none" # Refers to BertEmbeddings type (linear projection)
  layer_norm_eps: 1.0e-12 # Standard BERT value
  hidden_dropout_prob: 0.1 # Standard BERT value
  attention_probs_dropout_prob: 0.1 # Standard BERT value
  hidden_act: "gelu" # Standard BERT value

training:
  output_dir: "training_output" # Subdirectory under base_results_dir/run_name/
  batch_size: 256
  learning_rate: 5.0e-4
  epochs: 200
  patience_early_stopping: 200
  optimizer: "RAdam"
  loss_function: "MSELoss"
